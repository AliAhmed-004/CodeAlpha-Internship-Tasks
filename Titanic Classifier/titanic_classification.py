# -*- coding: utf-8 -*-
"""Titanic_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zC0cACqYI1ydqUoDcipOYmv5DJ0FXprL

# Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""# Loading the Training Data"""

titanic_data = pd.read_csv('train.csv')

"""# Creating a Heatmap"""

import seaborn as sns

string_columns = titanic_data.select_dtypes(include=['object']).columns

titanic_data_no_str = titanic_data.drop(columns=string_columns)

sns.heatmap(titanic_data_no_str.corr(), cmap='Reds', )

"""# Splitting the data to evenly distribute the data for training and testing"""

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2)

# We want even distributions for Survived, Pclass and Sex in the training and
# testing datasets
for train_indices, test_indices in split.split(titanic_data, titanic_data[['Survived', 'Pclass', 'Sex']]):
    strats_train_set = titanic_data.loc[train_indices]
    strats_test_set = titanic_data.loc[test_indices]

"""# Confirming if the data is split evenly"""

plt.subplot(1,2,1)
strats_train_set['Survived'].hist()
strats_train_set['Pclass'].hist()

plt.subplot(1,2,2)
strats_test_set['Survived'].hist()  #Blue
strats_test_set['Pclass'].hist()    #Orange

plt.show()

"""# Classes for handling missing data

## Handling Missing Age Data
"""

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer

class AgeImputer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.copy()
        imputer = SimpleImputer(strategy='mean')
        X['Age'] = imputer.fit_transform(X[['Age']])
        return X

"""## Encoding features to have numeric values for our model"""

from sklearn.preprocessing import OneHotEncoder

class FeatureEncoder(BaseEstimator, TransformerMixin):

    def fit(self, X, y=None):
        return self

    def  transform(self, X):
        encoder = OneHotEncoder()
        matrix = encoder.fit_transform(X[['Embarked']]).toarray()

        column_names = ['C', 'S', 'Q', 'N']

        for i in range(len(matrix.T)):
            X[column_names[i]] = matrix.T[i]

        matrix = encoder.fit_transform(X[['Sex']]).toarray()

        column_names = ['Female', 'Male']

        for i in range(len(matrix.T)):
            X[column_names[i]] = matrix.T[i]

        return X

"""## Dropping Features that are not important for our model"""

class FeatureDropper(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X.drop(['Name', 'Ticket', 'Cabin', 'Sex', 'Embarked', 'N'], axis = 1, errors='ignore')

"""# Creating a Pipeline for above methods"""

from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('ageimputer', AgeImputer()),
    ('featureencoder', FeatureEncoder()),
    ('featuredropper', FeatureDropper())
])

"""# Transforming the data to have a cleaner dataset"""

strats_train_set = pipeline.fit_transform(strats_train_set)

strats_train_set

"""# Confirming that there are no null values"""

strats_train_set.info()

"""# Scaling the data <br>
Since our model will predict if a passenger will survive or not (True or False), we have to scale the data so that we have either 0 or 1 rather than continuous values.


"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X = strats_train_set.drop(['Survived'], axis = 1)
y = strats_train_set['Survived']

X_train_data = scaler.fit_transform(X)
y_train_data = y.to_numpy()

"""# Training the Model"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Split the data into training and validation sets
X_train_set, X_val_set, y_train_set, y_val_set = train_test_split(X_train_data, y_train_data, test_size=0.2, random_state=42)

# Train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_set, y_train_set)

# Validate the model
y_val_pred = model.predict(X_val_set)
print("Validation Accuracy:", accuracy_score(y_val_set, y_val_pred))

"""# Preprocessing the Test Data"""

# Importing the Data
test_data = pd.read_csv('test.csv')

# Apply the pipeline to the test data
test_data_preprocessed = pipeline.transform(test_data)

# Extract features (no target column in test data)
X_test_data = test_data_preprocessed

"""#Checking if test data have any null values"""

X_test_data.info()

"""# Filling the null value in test data"""

X_test_data = X_test_data.fillna(method='ffill')

X_test_data.info()

"""# Making Predictions"""

test_predictions = model.predict(X_test_data)

test_predictions

submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': test_predictions})
submission.to_csv('titanic_predictions.csv', index=False)

